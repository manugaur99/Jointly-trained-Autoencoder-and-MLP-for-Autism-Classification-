{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "mWICoZD2ltJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9345673b-fb62-434d-f4a7-25a9e6aa5741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "p_ROI = \"cc200\"\n",
        "p_fold = 10\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a = torch.rand(8,4975)\n",
        "a\n",
        "a.shape"
      ],
      "metadata": {
        "id": "BWTCsLTFgOCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dba5eee-19be-4608-fcb7-dd4a57dcf65c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4975])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyprind"
      ],
      "metadata": {
        "id": "hxVVtfkUl4Yx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089cc57a-50e9-454d-a5d8-65f8ffd8c059"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameter_list = [p_ROI,p_fold]\n",
        "print(\"*****List of patameters****\")\n",
        "print(\"ROI atlas: \", p_ROI)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pyprind\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import stats\n",
        "from sklearn import tree\n",
        "import functools\n",
        "import numpy.ma as ma # for masked arrays\n",
        "import pyprind\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGI6jMOLgVrQ",
        "outputId": "4ad50ed4-3fa9-44d7-f616-8d316844d5e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****List of patameters****\n",
            "ROI atlas:  cc200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lla5FkGDltKA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHj2a9PjltKB"
      },
      "source": [
        "## Importing the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlJF1ubqltKD"
      },
      "outputs": [],
      "source": [
        "def get_key(filename):\n",
        "    f_split = filename.split('_')\n",
        "    if f_split[3] == 'rois':\n",
        "        key = '_'.join(f_split[0:3]) \n",
        "    else:\n",
        "        key = '_'.join(f_split[0:2])\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0iXwebRltKD",
        "outputId": "25e338e7-0b2e-46f3-da67-809a49aff284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n",
            "26\n",
            "{'OHSU_0050142': 1, 'OHSU_0050143': 1, 'OHSU_0050144': 1, 'OHSU_0050145': 1, 'OHSU_0050146': 1, 'OHSU_0050147': 1, 'OHSU_0050148': 1, 'OHSU_0050149': 1, 'OHSU_0050150': 1, 'OHSU_0050152': 1, 'OHSU_0050153': 1, 'OHSU_0050156': 1, 'OHSU_0050157': 0, 'OHSU_0050158': 0, 'OHSU_0050159': 0, 'OHSU_0050160': 0, 'OHSU_0050161': 0, 'OHSU_0050162': 0, 'OHSU_0050163': 0, 'OHSU_0050164': 0, 'OHSU_0050166': 0, 'OHSU_0050167': 0, 'OHSU_0050168': 0, 'OHSU_0050169': 0, 'OHSU_0050170': 0, 'OHSU_0050171': 0}\n"
          ]
        }
      ],
      "source": [
        "data_main_path = \"/content/drive/MyDrive/Abide_cc200/rois_\"+ p_ROI #cc200'#path to time series data\n",
        "flist = os.listdir(data_main_path)\n",
        "print(len(flist))\n",
        "\n",
        "for f in range(len(flist)):\n",
        "    flist[f] = get_key(flist[f])\n",
        "    \n",
        "\n",
        "df_labels = pd.read_csv(\"/content/drive/MyDrive/preprocessing ABIDE iran CNN/data/phenotypes/Phenotypic_V1_0b_preprocessed1.csv\")#path \n",
        "df_labels = df_labels[df_labels['FILE_ID'].str[0:4].isin([\"OHSU\"])] # only OHSU data\n",
        "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2:0})\n",
        "print(len(df_labels))\n",
        "\n",
        "labels = {}\n",
        "for row in df_labels.iterrows():\n",
        "    file_id = row[1]['FILE_ID']\n",
        "    y_label = row[1]['DX_GROUP']\n",
        "    if file_id == 'no_filename':\n",
        "        continue\n",
        "    assert(file_id not in labels)\n",
        "    labels[file_id] = y_label\n",
        "\n",
        "print(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47CRI7LLltKE"
      },
      "source": [
        "### Helper functions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62PxA093Br5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMu4zAzlltKF"
      },
      "outputs": [],
      "source": [
        "def get_label(filename):\n",
        "    assert (filename in labels)\n",
        "    return labels[filename]\n",
        "\n",
        "\n",
        "def get_corr_data(filename):\n",
        "    #print(filename)\n",
        "    for file in os.listdir(data_main_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "            \n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "        m = ma.masked_where(mask == 1, mask)\n",
        "        return ma.masked_where(m, corr).compressed()\n",
        "\n",
        "def get_corr_matrix(filename):\n",
        "    for file in os.listdir(data_main_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        return corr\n",
        "\n",
        "def confusion(g_turth,predictions):\n",
        "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
        "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
        "    sensitivity = (tp)/(tp+fn)\n",
        "    specificty = (tn)/(tn+fp)\n",
        "    return accuracy,sensitivity,specificty\n",
        "\n",
        "def get_regs(samplesnames,regnum):\n",
        "    datas = []\n",
        "    for sn in samplesnames:\n",
        "        datas.append(all_corr[sn][0])\n",
        "    datas = np.array(datas)\n",
        "    avg=[]\n",
        "    for ie in range(datas.shape[1]):\n",
        "        avg.append(np.mean(datas[:,ie]))\n",
        "    avg=np.array(avg)\n",
        "    highs=avg.argsort()[-regnum:][::-1]\n",
        "    lows=avg.argsort()[:regnum][::-1]\n",
        "    regions=np.concatenate((highs,lows),axis=0)\n",
        "    return regions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/drive/MyDrive/Abide_cc200/correlations_file'+p_ROI+'.pkl'):\n",
        "    pbar=pyprind.ProgBar(len(flist))\n",
        "    all_corr = {}\n",
        "    for f in flist:\n",
        "        lab = get_label(f)\n",
        "        all_corr[f] = (get_corr_data(f), lab)\n",
        "        pbar.update()\n",
        "\n",
        "    print('Corr-computations finished')\n",
        "    \n",
        "\n",
        "    pickle.dump(all_corr, open('/content/drive/MyDrive/Abide_cc200/correlations_file'+p_ROI+'.pkl', 'wb'))\n",
        "    print('Saving to file finished')\n",
        "\n",
        "else:\n",
        "    all_corr = pickle.load(open('/content/drive/MyDrive/Abide_cc200/correlations_file'+p_ROI+'.pkl', 'rb'))\n",
        "    "
      ],
      "metadata": {
        "id": "3aHhOBW_gLD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRELATION MATRIX UPPER TRIANGLE is A with an array of shape (2,)\n",
        "A = np.array(all_corr['OHSU_0050142'], dtype=object)\n",
        "#A[0].shape\n",
        "#all_corr['OHSU_0050142']\n",
        "timeseries = np.array(A[0])\n",
        "timeseries = timeseries.reshape(timeseries.shape[0],1)\n",
        "timeseries.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5oX_hOfhL6v",
        "outputId": "b65d23b4-08a1-4924-e9ba-4f4f6633c857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19900, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwsJMEl1ltKI"
      },
      "source": [
        "## Getting the correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FhAFLn-ltKI"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('/content/drive/MyDrive/Abide_cc200/correlations_file'+p_ROI+'.pkl'):\n",
        "    pbar=pyprind.ProgBar(len(flist))\n",
        "    all_corr = {}\n",
        "    for f in flist:\n",
        "        lab = get_label(f)\n",
        "        all_corr[f] = (get_corr_data(f), lab)\n",
        "        pbar.update()\n",
        "\n",
        "    print('Corr-computations finished')\n",
        "    \n",
        "\n",
        "    pickle.dump(all_corr, open('/content/drive/MyDrive/Abide_cc200/correlations_file'+p_ROI+'.pkl', 'wb'))\n",
        "    print('Saving to file finished')\n",
        "\n",
        "else:\n",
        "    all_corr = pickle.load(open('/content/drive/MyDrive/Abide_cc200/correlations_file'+p_ROI+'.pkl', 'rb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_correlations = pickle.load(open('/content/drive/MyDrive/temp data for ABIDE/correlations_file'+p_ROI+'.pkl', 'rb'))\n",
        "len(total_correlations.get('Caltech_0051456')[0])\n",
        "total_correlations.get('Caltech_0051456')[0].shape # (76636,) for cc400(4X)\n",
        "#len(total_correlations)"
      ],
      "metadata": {
        "id": "Y_pSbrH5mMvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556e23d3-c5f5-4dd8-ae85-2c5a6ed9f607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19900,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW8Jjup9ltKJ"
      },
      "source": [
        "## Computing eigenvalues and eigenvector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOJL12ealtKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c762864d-3712-46eb-9741-d3436d2dea74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##########################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:06\n"
          ]
        }
      ],
      "source": [
        "eig_data = {}\n",
        "pbar = pyprind.ProgBar(len(flist))\n",
        "for f in flist:  \n",
        "    d = get_corr_matrix(f)\n",
        "    eig_vals, eig_vecs = np.linalg.eig(d)\n",
        "\n",
        "    for ev in eig_vecs.T:\n",
        "        np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
        "\n",
        "    sum_eigvals = np.sum(np.abs(eig_vals))\n",
        "    # Make a list of (eigenvalue, eigenvector, norm_eigval) tuples\n",
        "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i], np.abs(eig_vals[i])/sum_eigvals)\n",
        "                  for i in range(len(eig_vals))]\n",
        "\n",
        "    # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    eig_data[f] = {'eigvals':np.array([ep[0] for ep in eig_pairs]),\n",
        "                    'norm-eigvals':np.array([ep[2] for ep in eig_pairs]),\n",
        "                    'eigvecs':[ep[1] for ep in eig_pairs]}\n",
        "    pbar.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrM3urESltKL"
      },
      "source": [
        "## Defining dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tFY44FpltKL"
      },
      "outputs": [],
      "source": [
        "class CC200Dataset(Dataset):\n",
        "    def __init__(self, pkl_filename=None, data=None, samples_list=None, \n",
        "                 augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, verbose=False,regs=None):\n",
        "        self.regs=regs\n",
        "        if pkl_filename is not None:\n",
        "            if verbose:\n",
        "                print ('Loading ..!', end=' ')\n",
        "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
        "        elif data is not None:\n",
        "            self.data = data.copy()\n",
        "            \n",
        "        else:\n",
        "            sys.stderr.write('Eigther PKL file or data is needed!')\n",
        "            return \n",
        "\n",
        "        #if verbose:\n",
        "        #    print ('Preprocess..!', end='  ')\n",
        "        if samples_list is None:\n",
        "            self.flist = [f for f in self.data]\n",
        "        else:\n",
        "            self.flist = [f for f in samples_list]\n",
        "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
        "        \n",
        "        current_flist = np.array(self.flist.copy())\n",
        "        current_lab0_flist = current_flist[self.labels == 0]\n",
        "        current_lab1_flist = current_flist[self.labels == 1]\n",
        "        #if verbose:\n",
        "        #    print(' Num Positive : ', len(current_lab1_flist), end=' ')\n",
        "        #    print(' Num Negative : ', len(current_lab0_flist), end=' ')\n",
        "        \n",
        "        \n",
        "\n",
        "        self.num_data = len(self.flist)\n",
        "\n",
        "        # returns X and Y \n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.flist):\n",
        "            fname = self.flist[index]\n",
        "            data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode)    \n",
        "            data = data[self.regs].copy()\n",
        "            label = (self.labels[index],)\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "        else:\n",
        "            f1 = self.flist[index % len(self.flist)]\n",
        "            d1, y1 = self.data[f1][0], self.data[f1][1]\n",
        "            d1=d1[self.regs]\n",
        "            f2 = np.random.choice(self.neighbors[f1])\n",
        "            d2, y2 = self.data[f2][0], self.data[f2][1]\n",
        "            d2=d2[self.regs]\n",
        "            assert y1 == y2\n",
        "            r = np.random.uniform(low=0, high=1)\n",
        "            label = (y1,)\n",
        "            data = r*d1 + (1-r)*d2\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8lIW0DSltKM"
      },
      "source": [
        "## Definig data loader function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9qm-xVcltKM"
      },
      "outputs": [],
      "source": [
        "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
        "               batch_size=64, \n",
        "               num_workers=1, mode='train',\n",
        "               *, augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, verbose=False,regions=None):\n",
        "    \"\"\"Build and return data loader.\"\"\"\n",
        "    if mode == 'train':\n",
        "        shuffle = True\n",
        "    else:\n",
        "        shuffle = False\n",
        "        augmentation=False\n",
        "\n",
        "    dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list,\n",
        "                           augmentation=augmentation, aug_factor=aug_factor, \n",
        "                           eig_data=eig_data, verbose=verbose,regs=regions)\n",
        "\n",
        "    data_loader = DataLoader(dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlhYxWhxltKN"
      },
      "source": [
        "## Defining Autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mR6eRmtltKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43a0344-1f99-45e8-c16f-c9ed0242f115"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MTAutoEncoder(\n",
              "  (fc_encoder): Linear(in_features=990, out_features=200, bias=True)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
              "    (1): Linear(in_features=100, out_features=20, bias=True)\n",
              "    (2): Linear(in_features=20, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "class MTAutoEncoder(nn.Module):\n",
        "    def __init__(self, num_inputs=990, \n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(MTAutoEncoder, self).__init__()\n",
        "        self.tied = tied\n",
        "        self.num_latent = num_latent\n",
        "        \n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent) # encoder : 990 > 200\n",
        "        #decoder if weights not tied\n",
        "        if tied== False:  \n",
        "            self.fc_decoder = nn.Linear(num_latent, num_inputs) \n",
        "         \n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "        #SLP with AE bottleneck as input \n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Dropout(p=0.4), # prob a neuron will be zeroed\n",
        "                nn.Linear(self.num_latent, int(self.num_latent/2)),\n",
        "                #nn.ReLU(),\n",
        "                nn.Linear(int(self.num_latent/2), int(self.num_latent/10)),\n",
        "                #nn.ReLU(),\n",
        "                nn.Linear(int(self.num_latent/10), 1),                \n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Linear(self.num_latent, int((self.num_latent)/2)),\n",
        "                #nn.ReLU(),\n",
        "                nn.Linear(int(self.num_latent/2), int(self.num_latent/10)),\n",
        "                #nn.ReLU(),\n",
        "                nn.Linear(int(self.num_latent/10), 1),\n",
        "            )\n",
        "            \n",
        "         \n",
        "    def forward(self, x, eval_classifier=False):\n",
        "        x = self.fc_encoder(x)\n",
        "        x = torch.tanh(x)\n",
        "        if eval_classifier:\n",
        "            x_logit = self.classifier(x)\n",
        "        else:\n",
        "            x_logit = None # initialised\n",
        "        \n",
        "        if self.tied: \n",
        "            x = F.linear(x, self.fc_encoder.weight.t())\n",
        "        else:\n",
        "            x = self.fc_decoder(x)\n",
        "            \n",
        "        return x, x_logit\n",
        "\n",
        "mtae = MTAutoEncoder()\n",
        "\n",
        "mtae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jLn5OqltKO"
      },
      "source": [
        "## Defining training and testing functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCwkegGEltKO"
      },
      "outputs": [],
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
        "    model.train()  \n",
        "    train_losses = []\n",
        "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
        "        if len(batch_x) != batch_size:  # batch_size can't be equal to training size\n",
        "            continue\n",
        "        if p_bernoulli is not None:\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        data, target = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mode in ['both', 'ae']:\n",
        "            if p_bernoulli is not None:\n",
        "                rec_noisy, _ = model(data*rand_bernoulli, False)\n",
        "                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n",
        "            else:\n",
        "                rec, _ = model(data, False)\n",
        "                loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
        "\n",
        "        if mode in ['both', 'clf']:\n",
        "            rec_clean, logits = model(data, True)\n",
        "            loss_clf = criterion_clf(logits, target)\n",
        "\n",
        "        if mode == 'both':\n",
        "            loss_total = loss_ae + lam_factor*loss_clf\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "        elif mode == 'ae':\n",
        "            loss_total = loss_ae\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
        "                                 0.0])\n",
        "        elif mode == 'clf':\n",
        "            loss_total = loss_clf\n",
        "            train_losses.append([0.0, \n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return train_losses       \n",
        "\n",
        "def test(model, criterion, test_loader, \n",
        "         eval_classifier=False, num_batch=None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    all_predss=[]\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i,(batch_x,batch_y) in enumerate(test_loader, 1):\n",
        "            if num_batch is not None:\n",
        "                if i >= num_batch:\n",
        "                    continue\n",
        "            data = batch_x.to(device)\n",
        "            rec, logits = model(data, eval_classifier)\n",
        "\n",
        "            test_loss += criterion(rec, data).detach().cpu().numpy() \n",
        "            n_test += len(batch_x)\n",
        "            if eval_classifier:\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)###????\n",
        "                y_arr = np.array(batch_y, dtype=np.int32)\n",
        "\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n",
        "\n",
        "    return  mlp_acc,mlp_sens,mlp_spef#,correct/n_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhT_m_DQltKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfc3e06-c1ce-495c-8d8d-a4c880818375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxdCwOlUEfdn",
        "outputId": "61cf894d-a6d7-4a71-c07f-fe50a9260989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 31 09:43:18 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    58W / 149W |    783MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "m4sdcvatltKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11faeed-4b97-4973-e31d-f3c112e6e9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_corr:   19900\n",
            "this is n_lat 4975\n",
            "p_bernoulli:  None\n",
            "augmentaiton:  True aug_factor:  2 num_neighbs:  5 lim4sim:  2\n",
            "use_dropout:  True \n",
            "\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.3333333333333333, 0.5, 0.0)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "averages:\n",
            "[0.81666667 0.85       0.8       ]\n",
            "94.32975840568542\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 0.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "averages:\n",
            "[0.85  0.825 0.875]\n",
            "181.67562246322632\n",
            "(0.6666666666666666, 0.5, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.5, 1.0, 0.0)\n",
            "averages:\n",
            "[0.85       0.86666667 0.85      ]\n",
            "269.14678168296814\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 0.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "(0.5, 1.0, 0.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "averages:\n",
            "[0.85416667 0.85       0.8625    ]\n",
            "355.9482614994049\n",
            "(0.6666666666666666, 0.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(0.6666666666666666, 1.0, 0.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "averages:\n",
            "[0.85666667 0.86       0.85      ]\n",
            "443.1910169124603\n",
            "(0.6666666666666666, 0.5, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 0.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.5, 1.0, 0.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "averages:\n",
            "[0.86111111 0.85833333 0.85833333]\n",
            "530.873920917511\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 0.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(0.6666666666666666, 0.5, 1.0)\n",
            "(0.6666666666666666, 0.5, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "averages:\n",
            "[0.86190476 0.85       0.87142857]\n",
            "618.5222046375275\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(0.6666666666666666, 1.0, 0.5)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.6666666666666666, 0.5, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "averages:\n",
            "[0.86041667 0.85       0.875     ]\n",
            "707.5501382350922\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.3333333333333333, 0.0, 0.5)\n",
            "(0.6666666666666666, 0.5, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.5, 1.0, 0.0)\n",
            "(0.5, 1.0, 0.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "averages:\n",
            "[0.84814815 0.83888889 0.86111111]\n",
            "796.8021974563599\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(0.3333333333333333, 0.5, 0.0)\n",
            "(0.6666666666666666, 0.5, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "(1.0, 1.0, 1.0)\n",
            "averages:\n",
            "[0.85333333 0.845      0.865     ]\n",
            "885.6109628677368\n",
            "Avergae result of 10 repeats:  [0.8512414  0.84938889 0.8568373 ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "num_corr = len(all_corr[flist[0]][0])\n",
        "print(\"num_corr:  \",num_corr)\n",
        "\n",
        "start =time.time()\n",
        "batch_size = 8   #orginal = 8 \n",
        "learning_rate_ae, learning_rate_clf = 0.0001, 0.001\n",
        "num_epochs = 25   #original 25\n",
        "\n",
        "p_bernoulli = None\n",
        "use_dropout = True\n",
        "augmentation = True\n",
        "\n",
        "aug_factor = 2\n",
        "num_neighbs = 5\n",
        "lim4sim = 2\n",
        "n_lat = int(num_corr/4)\n",
        "print(\"this is n_lat\",n_lat)\n",
        "start= time.time()\n",
        "\n",
        "print('p_bernoulli: ', p_bernoulli)\n",
        "print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "      'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "crossval_res_kol=[]\n",
        "all_rp_res = []\n",
        "y_arr = np.array([get_label(f) for f in flist])\n",
        "flist = np.array(flist)\n",
        "kk=0 \n",
        "for rp in range(10):\n",
        "    kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "    np.random.shuffle(flist)\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "    for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "        train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "\n",
        "        verbose = (True if (kk == 0) else False)\n",
        "\n",
        "        regions_inds = get_regs(train_samples,int(num_corr/4))\n",
        "\n",
        "        num_inpp = len(regions_inds)\n",
        "        n_lat = int(num_inpp/2)\n",
        "        train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
        "                                batch_size=batch_size, mode='train',\n",
        "                                augmentation=augmentation, aug_factor=aug_factor, \n",
        "                                num_neighbs=num_neighbs, eig_data=eig_data, \n",
        "                                verbose=verbose,regions=regions_inds)\n",
        "    #shape of train loader : image = [8,9950] and label = [8,1]\n",
        "\n",
        "        test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
        "                                batch_size=batch_size, mode='test', augmentation=False, \n",
        "                                verbose=verbose,regions=regions_inds)\n",
        "    #shape of test loader : image = [3,9950] and label = [3,1]\n",
        "\n",
        "        model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "        model.to(device)\n",
        "        criterion_ae = nn.MSELoss(reduction='sum')\n",
        "        criterion_clf = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                              momentum=0.9)\n",
        "\n",
        "        for epoch in range(1, num_epochs+1):\n",
        "            if epoch <= 20:\n",
        "                train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "            else:\n",
        "                train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "\n",
        "        res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "        \n",
        "        print(test(model, criterion_ae, test_loader, eval_classifier=True))\n",
        "        crossval_res_kol.append(res_mlp)\n",
        "\n",
        "    print(\"averages:\")\n",
        "    print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "    all_rp_res.append(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "    finish= time.time()\n",
        "\n",
        "    print(finish-start)\n",
        "print(\"Avergae result of 10 repeats: \",np.mean(np.array(all_rp_res),axis = 0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKAOh3tSltKP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxB28OYtltKQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgSzONjVltKQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0WQs5OOltKQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vavDxSXOltKQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bCOT7PzltKQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}